
Logistic regresioon with lasso penalty
The best model is: ['The tuned tfifdvectorizer model ', 0.8]
With hyperparameters Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(ngram_range=(1, 2))),
                ('logisticregression',
                 LogisticRegression(C=10.0, max_iter=5000, penalty='l1',
                                    solver='saga'))])
accuracy:  0.8
Table:                precision    recall  f1-score   support

           0       0.80      0.80      0.80        80
           1       0.80      0.80      0.80        80

    accuracy                           0.80       160
   macro avg       0.80      0.80      0.80       160
weighted avg       0.80      0.80      0.80       160

confusion matrix:  [[64 16]
 [16 64]]
Top positive words:
millennium: 25.303
recently: 23.494
luxury: 21.695
finally: 20.090
decided: 17.936
smell: 16.776
seemed: 16.257
smelled: 15.060
recent: 14.814
turned: 14.446

Top negative words:
star: -20.015
security: -16.204
rate: -15.728
location: -14.262
world: -13.130
called: -13.065
tiny: -12.517
floor: -11.980
construction: -11.140
returned: -11.113


Multinomial Naive Bayes
The best model is: ['The tuned tfifdvectorizer model ', 0.8875]
With hyperparameters Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(ngram_range=(1, 2))),
                ('multinomialnb', MultinomialNB(alpha=0.5))])
accuracy:  0.8875
Table:                precision    recall  f1-score   support

           0       0.94      0.82      0.88        80
           1       0.84      0.95      0.89        80

    accuracy                           0.89       160
   macro avg       0.89      0.89      0.89       160
weighted avg       0.89      0.89      0.89       160

confusion matrix:  [[66 14]
 [ 4 76]]

Top 10 for class 0 (index 0):
great                0.000197
location             0.000184
bed                  0.000181
day                  0.000163
never                0.000158
good                 0.000155
called               0.000153
floor                0.000153
nice                 0.000153
front                0.000149

Top 10 for class 1 (index 1):
experience           0.000181
time                 0.000175
front                0.000175
got                  0.000171
could                0.000169
finally              0.000157
arrived              0.000154
first                0.000154
rude                 0.000153
check                0.000153


Classification Tree

Chosen hyperparemeters with highest accuracy value: {'max_depth': nan, 'min_samples_leaf': 1.0, 'ccp_alpha': 0.01, 'test_accuracy': 0.66875}
accuracy:  0.66875
Table:                precision    recall  f1-score   support

           0       0.65      0.72      0.69        80
           1       0.69      0.61      0.65        80

    accuracy                           0.67       160
   macro avg       0.67      0.67      0.67       160
weighted avg       0.67      0.67      0.67       160

confusion matrix:  [[58 22]
 [31 49]]

Top 10 Most Important Features ClassificationTree:
         feature  importance
3380    location    0.169484
3501      luxury    0.148919
5370       smell    0.109223
2046  experience    0.094426
3698  millennium    0.084880
1570     decided    0.074904
2225     finally    0.073533
5122      seemed    0.068023
1140     cleaned    0.066963
6040      towels    0.055594


Randomforest
Shape of INPUT: (640, 5000)
accuracy:  0.83125
Table:                precision    recall  f1-score   support

           0       0.83      0.84      0.83        80
           1       0.84      0.82      0.83        80

    accuracy                           0.83       160
   macro avg       0.83      0.83      0.83       160
weighted avg       0.83      0.83      0.83       160

confusion matrix:  [[67 13]
 [14 66]]

Top 10 Most Important Features RandomForest:
       feature  importance
2385  location    0.016642
1418   finally    0.014558
4066     smell    0.013214
2508    luxury    0.012882
3806    seemed    0.011696
3471  recently    0.010893
1074   decided    0.008864
1637     great    0.008355
247    arrived    0.007991
3668      rude    0.007936


GradBoost
Shape of INPUT: (640, 5000)
accuracy:  0.8125
Table:                precision    recall  f1-score   support

           0       0.82      0.80      0.81        80
           1       0.80      0.82      0.81        80

    accuracy                           0.81       160
   macro avg       0.81      0.81      0.81       160
weighted avg       0.81      0.81      0.81       160

confusion matrix:  [[64 16]
 [14 66]]

Top 10 Most Important Features:
         feature  importance
2385    location    0.019218
4066       smell    0.015145
1418     finally    0.014868
3471    recently    0.014854
2508      luxury    0.013591
1074     decided    0.011450
3806      seemed    0.011287
3668        rude    0.009057
1328  experience    0.008981
2720  millennium    0.008909