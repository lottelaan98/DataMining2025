
THE LOGISTIC REGRESIOON model
UNIGRAM - LEMMETIZATION AND STEMMING

The best model is: ['The tuned tfifdvectorizer model ', 0.8375]
With hyperparameters Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),
                ('logisticregression',
                 LogisticRegression(C=10.0, max_iter=5000, penalty='l1',
                                    solver='saga'))])
accuracy:  0.8375
Table:                precision    recall  f1-score   support

           0       0.81      0.89      0.85        80
           1       0.88      0.79      0.83        80

    accuracy                           0.84       160
   macro avg       0.84      0.84      0.84       160
weighted avg       0.84      0.84      0.84       160

confusion matrix:  [[71  9]
 [17 63]]
Top positive words:
recent: 21.099
smell: 18.824
luxuri: 17.954
millennium: 16.664
final: 13.540
homewood: 13.093
expect: 11.161
seem: 10.905
heart: 9.998
make: 9.846

Top negative words:
world: -13.166
star: -11.552
secur: -10.893
call: -10.462
elev: -10.020
cool: -9.298
self: -8.748
date: -8.658
confer: -8.390
bed: -8.263

---------------------------------------------------------------------------------------------

Multinomial Naive Bayes
with unigram and LEMMETIZATION

The best model is: ['The tuned tfifdvectorizer model ', 0.85625]
With hyperparameters Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),
                ('multinomialnb', MultinomialNB(alpha=0.5))])
accuracy:  0.85625
Table:                precision    recall  f1-score   support

           0       0.90      0.80      0.85        80
           1       0.82      0.91      0.86        80

    accuracy                           0.86       160
   macro avg       0.86      0.86      0.86       160
weighted avg       0.86      0.86      0.86       160

confusion matrix:  [[64 16]
 [ 7 73]]

Top 10 for class 0 (index 0):
bed                  0.001818
great                0.001636
location             0.001545
time                 0.001494
day                  0.001477
floor                0.001378
never                0.001272
good                 0.001267
elevator             0.001263
nice                 0.001256

Top 10 for class 1 (index 1):
time                 0.001605
experience           0.001604
front                0.001474
got                  0.001447
could                0.001412
bed                  0.001405
finally              0.001317
hour                 0.001312
rude                 0.001302
check                0.001297

---------------------------------------------------------------------------------------------

Classification Tree
with unigram and stemming

Chosen hyperparemeters with highest accuracy value: {'max_depth': nan, 'min_samples_leaf': 2.0, 'ccp_alpha': 0.0, 'test_accuracy': 0.7125}
accuracy:  0.7125
Table:                precision    recall  f1-score   support

           0       0.71      0.71      0.71        80
           1       0.71      0.71      0.71        80

    accuracy                           0.71       160
   macro avg       0.71      0.71      0.71       160
weighted avg       0.71      0.71      0.71       160

confusion matrix:  [[57 23]
 [23 57]]

Top 10 Most Important Features ClassificationTree:
         feature  importance
2072       smell    0.091678
865        final    0.065776
1341      luxuri    0.064686
1805      recent    0.059334
1918        rude    0.039223
135       around    0.032177
1415  millennium    0.030309
1554       onlin    0.023773
1096        hope    0.019585
925     frequent    0.018439

---------------------------------------------------------------------------------------------

Randomforest
Uni and bigram with LEMMETIZATION and stemming

Shape of INPUT: (640, 5000)
accuracy:  0.8625
Table:                precision    recall  f1-score   support

           0       0.84      0.90      0.87        80
           1       0.89      0.82      0.86        80

    accuracy                           0.86       160
   macro avg       0.86      0.86      0.86       160
weighted avg       0.86      0.86      0.86       160

confusion matrix:  [[72  8]
 [14 66]]

Top 10 Most Important Features RandomForest:
         feature  importance
3997       smell    0.016225
3479      recent    0.014550
2470      luxuri    0.013002
2335       locat    0.011878
1259        elev    0.011216
3749        seem    0.010441
1428       final    0.010378
2680  millennium    0.009223
2390        look    0.008587
701         call    0.008102


---------------------------------------------------------------------------------------------

GradBoost
Uni and bigram with LEMMETIZATION and stemming

Shape of INPUT: (640, 5000)
accuracy:  0.86875
Table:                precision    recall  f1-score   support

           0       0.83      0.93      0.88        80
           1       0.92      0.81      0.86        80

    accuracy                           0.87       160
   macro avg       0.87      0.87      0.87       160
weighted avg       0.87      0.87      0.87       160

confusion matrix:  [[74  6]
 [15 65]]

Top 10 Most Important Features:
     feature  importance
3997   smell    0.017268
2470  luxuri    0.016934
3479  recent    0.015281
2335   locat    0.014987
1428   final    0.011677
3749    seem    0.011615
1259    elev    0.011370
2390    look    0.009876
1335  expect    0.009827
3635    rude    0.009470